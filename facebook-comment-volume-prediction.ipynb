{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToFile = r\"./dataset/Training/\"\n",
    "\n",
    "training_files = ['Features_Variant_1.csv', 'Features_Variant_2.csv', 'Features_Variant_3.csv', \n",
    "                 'Features_Variant_4.csv', 'Features_Variant_5.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train = pd.concat([pd.read_csv(pathToFile + f, index_col=False, header=None) for f in training_files ], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = combined_train.iloc[:,:-1]\n",
    "y_train_all = combined_train.iloc[:,-1:]\n",
    "\n",
    "X_train_list = []\n",
    "y_train_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    X_train_list.append(pd.read_csv(pathToFile + training_files[i], index_col=False, header=None).iloc[:,:-1])\n",
    "    y_train_list.append(pd.read_csv(pathToFile + training_files[i], index_col=False, header=None).iloc[:,-1:])\n",
    "\n",
    "X_train_list.append(X_train_all)\n",
    "y_train_list.append(y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToTestFile = r\"./dataset/Testing/TestSet/\"\n",
    "\n",
    "test_files = ['Test_Case_1.csv', 'Test_Case_2.csv', 'Test_Case_3.csv',\n",
    "              'Test_Case_4.csv', 'Test_Case_5.csv', 'Test_Case_5.csv', \n",
    "              'Test_Case_6.csv', 'Test_Case_7.csv', 'Test_Case_8.csv', \n",
    "              'Test_Case_9.csv', 'Test_Case_10.csv']\n",
    "for f in test_files:\n",
    "    test = pd.concat([pd.read_csv(pathToTestFile + f)])\n",
    "X_test = test.iloc[:,:-1]\n",
    "y_test = test.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 1:  76.29570764094876\n",
      "Variant 2:  69.47673578082606\n",
      "Variant 3:  65.81800233065375\n",
      "Variant 4:  74.23468686798097\n",
      "Variant 5:  88.85623287053683\n",
      "Variant overall:  69.00928537042391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "for i in range(6):\n",
    "    lin_reg.fit(X_train_list[i], y_train_list[i])\n",
    "    \n",
    "    rms = sqrt(mean_squared_error(y_test, lin_reg.predict(X_test)))\n",
    "    if i == 5:\n",
    "        print(\"Variant overall: \", rms)\n",
    "    else:\n",
    "        print(\"Variant {0}: \".format(i + 1), rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 1:  1399.5406542606897 [1.94177032] [[ 9.82688085e-03  5.78036115e-04  2.54493904e-05 ... -1.06843735e-02\n",
      "   0.00000000e+00 -7.29756707e-04]]\n",
      "Variant 2:  3096.5671194693928 [5.95246589] [[-1.26932819e-03 -6.49879157e-04 -1.82654149e-05 ... -4.33965150e-03\n",
      "   0.00000000e+00 -2.47517504e-03]]\n",
      "Variant 3:  866.359398131891 [5.43732362] [[-1.72527681e-02  4.78338371e-04 -2.55912635e-07 ... -6.51019381e-03\n",
      "   0.00000000e+00  1.88164807e-03]]\n",
      "Variant 4:  48524.618332151455 [2.21880765] [[-8.88073527e-03 -7.49891140e-05  2.81679428e-05 ... -3.18319966e-03\n",
      "   0.00000000e+00  2.35730624e-04]]\n",
      "Variant 5:  8186.991439661674 [2.53890096] [[-1.97618950e-03  1.46882713e-04  1.16342662e-05 ... -4.05670710e-03\n",
      "   0.00000000e+00 -9.38078474e-04]]\n",
      "Variant overall:  94.62786225543991 [4.92105257] [[-7.04186909e-03  5.64920175e-04 -3.23562319e-05 ... -3.80530038e-03\n",
      "   0.00000000e+00 -1.04229412e-04]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=True)\n",
    "lin_reg = LinearRegression()\n",
    "for i in range(6):\n",
    "    X_poly = poly_features.fit_transform(X_train_list[i])\n",
    "    lin_reg.fit(X_poly, y_train_list[i])\n",
    "    \n",
    "    X_test_poly = poly_features.fit_transform(X_test)\n",
    "    y_predicted = lin_reg.predict(X_test_poly)\n",
    "    rms = sqrt(mean_squared_error(y_test, y_predicted))\n",
    "\n",
    "    if i == 5:\n",
    "        print(\"Variant overall: \", rms, lin_reg.intercept_, lin_reg.coef_)\n",
    "    else:\n",
    "        print(\"Variant {0}: \".format(i + 1), rms, lin_reg.intercept_, lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 1:  99.69758326175952 [7.10522935e-03 1.21425523e-02 9.89321643e-03 1.13371940e-02\n",
      " 1.34166654e-04 6.14060081e-03 1.48761216e-02 3.47381769e-03\n",
      " 2.34298709e-03 1.75964554e-04 1.00911043e-03 8.00682887e-03\n",
      " 1.01233814e-02 1.90309610e-02 0.00000000e+00 1.18250556e-02\n",
      " 6.83725878e-03 7.19840556e-03 2.75172229e-03 7.57892334e-05\n",
      " 1.67230150e-02 2.91869807e-03 1.60526668e-03 3.88364063e-02\n",
      " 4.05229340e-03 1.16449715e-03 1.86031088e-03 1.42324242e-03\n",
      " 8.83734779e-04 5.18112926e-02 2.81098928e-01 7.69487112e-04\n",
      " 6.23671399e-03 1.17680241e-02 2.48262054e-01 1.62288597e-02\n",
      " 1.46596285e-01 0.00000000e+00 3.35569255e-04 2.09276845e-03\n",
      " 1.32987521e-03 5.86389357e-03 1.01603015e-02 5.61419168e-03\n",
      " 1.21300260e-03 1.13000363e-03 2.64939503e-03 3.69789789e-04\n",
      " 2.45514464e-04 2.40075242e-04 3.80436178e-04 1.19752259e-03\n",
      " 4.58188580e-04]\n",
      "Variant 2:  90.26985555483283 [5.56067630e-03 7.66645016e-03 5.62384363e-02 8.85395213e-03\n",
      " 1.16144275e-03 3.52265094e-02 7.74696614e-02 8.23130977e-03\n",
      " 5.76360012e-03 1.70615281e-03 6.02496127e-04 1.85212084e-03\n",
      " 6.17227640e-03 4.99327542e-03 0.00000000e+00 9.42781666e-03\n",
      " 2.30539946e-03 1.07913658e-02 1.04661149e-02 1.19840779e-06\n",
      " 1.63310568e-03 2.09815678e-03 7.90838923e-03 1.00033369e-03\n",
      " 1.55030522e-02 1.06985137e-02 6.33247037e-03 1.54052330e-02\n",
      " 1.04279471e-03 1.15336380e-02 2.67701842e-01 2.12092017e-04\n",
      " 7.89598647e-03 2.22803437e-02 2.23516478e-01 3.00128367e-02\n",
      " 1.06718905e-01 0.00000000e+00 2.91738245e-03 2.70420567e-04\n",
      " 6.20498595e-04 3.47923400e-03 1.33314761e-04 5.37330252e-04\n",
      " 2.82481031e-03 7.13818219e-05 6.99480781e-04 2.04010213e-04\n",
      " 1.74003335e-04 2.46138247e-04 6.38155448e-04 7.05955322e-04\n",
      " 4.93457970e-04]\n",
      "Variant 3:  69.01317288976026 [5.68008837e-03 1.35190582e-02 8.01050754e-03 7.22003898e-03\n",
      " 2.38501647e-04 6.66816531e-03 3.69479014e-03 3.89218967e-03\n",
      " 1.01889822e-02 1.80873242e-04 2.47359291e-03 2.22089818e-02\n",
      " 2.67100153e-02 8.29347947e-03 0.00000000e+00 1.81785884e-03\n",
      " 2.70319323e-03 6.10715158e-03 5.49413491e-03 1.52911356e-04\n",
      " 8.51843282e-03 7.67756398e-02 7.15113412e-03 3.18267114e-03\n",
      " 4.26057403e-03 1.16888454e-03 1.94905215e-02 1.18319044e-02\n",
      " 3.77384378e-03 4.57573372e-02 2.99343008e-01 5.57575952e-04\n",
      " 1.00809522e-02 1.58089867e-02 2.14614830e-01 1.63319784e-02\n",
      " 9.27519902e-02 0.00000000e+00 6.04005203e-04 3.60586594e-04\n",
      " 3.97990320e-03 1.12078714e-03 2.01343629e-04 2.17338103e-04\n",
      " 1.56475576e-03 6.00915390e-04 1.57883903e-03 5.54100903e-04\n",
      " 1.15282637e-02 1.31936334e-03 1.04080559e-04 9.47622957e-03\n",
      " 1.34708301e-04]\n",
      "Variant 4:  124.71699017658327 [1.82947758e-02 8.90243837e-03 2.37132358e-03 7.73034345e-03\n",
      " 3.30358761e-03 4.04857085e-03 5.18086848e-02 7.46335508e-03\n",
      " 1.29808865e-02 2.96491900e-05 5.26090846e-03 4.54841827e-03\n",
      " 1.11916987e-02 1.69142914e-02 0.00000000e+00 6.59323145e-03\n",
      " 2.62946982e-03 5.19838695e-03 3.74384254e-03 5.12674796e-05\n",
      " 1.54047876e-02 2.40052113e-02 3.06894188e-03 9.74546659e-04\n",
      " 8.45039135e-03 4.73272429e-04 8.69592313e-03 4.03978421e-03\n",
      " 1.31628928e-03 1.40778335e-02 2.94422562e-01 2.96604096e-04\n",
      " 1.33651122e-02 4.30674008e-02 2.25057645e-01 3.12400223e-02\n",
      " 1.17981383e-01 0.00000000e+00 3.01431779e-04 6.86714260e-04\n",
      " 2.16192258e-03 8.42155083e-04 5.07851621e-04 1.88312127e-03\n",
      " 2.03672967e-04 1.32743638e-03 1.25711766e-03 2.00172204e-03\n",
      " 1.70179562e-03 4.06050204e-03 5.08856770e-04 5.89802377e-04\n",
      " 2.96305648e-03]\n",
      "Variant 5:  82.99734588082623 [7.43460382e-03 7.97385894e-03 7.79443278e-03 6.60449788e-03\n",
      " 5.17848947e-05 1.80423042e-02 6.57579485e-02 4.15771077e-03\n",
      " 1.43642724e-03 1.91384899e-04 3.55290803e-03 3.79699367e-03\n",
      " 8.31033872e-03 1.13731263e-02 0.00000000e+00 3.25140957e-03\n",
      " 4.02750443e-02 5.74210672e-03 2.44839345e-03 8.00804960e-04\n",
      " 4.18571798e-03 7.40613613e-03 8.86356278e-03 1.29103879e-03\n",
      " 2.16551250e-03 3.88659943e-03 4.36120900e-03 5.84635779e-03\n",
      " 8.65824923e-03 1.80914936e-02 3.07503673e-01 3.19334033e-04\n",
      " 2.40570896e-02 6.50517242e-02 2.20665815e-01 1.66067411e-02\n",
      " 8.23761908e-02 0.00000000e+00 2.89076548e-04 8.35629633e-04\n",
      " 2.48408246e-03 1.35537698e-03 2.87918837e-04 2.59542596e-04\n",
      " 1.80046552e-03 2.91684841e-04 5.24930014e-04 5.22853236e-04\n",
      " 3.86127868e-03 1.20718098e-03 3.46352570e-03 3.11436183e-04\n",
      " 2.17249405e-03]\n",
      "Variant overall:  60.52922905258934 [8.05134683e-03 9.18371820e-03 1.11376176e-02 6.56414193e-03\n",
      " 1.93211833e-04 2.06259080e-02 7.87903360e-02 5.43066441e-03\n",
      " 2.64802819e-03 4.80719623e-05 5.76853498e-03 3.04830008e-02\n",
      " 5.84240917e-03 5.90358857e-03 0.00000000e+00 1.58938930e-03\n",
      " 2.63254526e-03 6.54076784e-03 7.92880715e-03 2.06589319e-04\n",
      " 8.80814755e-03 2.30308151e-03 5.59251863e-03 4.07504016e-03\n",
      " 5.46232857e-03 1.17792643e-03 5.30527559e-03 6.39475046e-03\n",
      " 1.09007917e-02 7.79988187e-03 3.39252939e-01 2.96653200e-04\n",
      " 1.13949542e-02 1.85963414e-02 2.31245737e-01 2.81068065e-02\n",
      " 8.99693164e-02 0.00000000e+00 5.19147921e-04 9.06713899e-05\n",
      " 1.88077444e-03 1.45848342e-03 7.14349065e-04 3.05333826e-04\n",
      " 8.00595851e-04 1.43315330e-03 8.07682630e-04 6.19101793e-04\n",
      " 1.00485768e-03 2.32918624e-03 4.43901725e-04 9.67273463e-04\n",
      " 3.74319240e-04]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=12, random_state=42)\n",
    "\n",
    "for i in range(6):\n",
    "    tree_reg.fit(X_train_list[i], y_train_list[i])\n",
    "    y_predicted = tree_reg.predict(X_test)\n",
    "    rms = sqrt(mean_squared_error(y_test, y_predicted))\n",
    "\n",
    "    if i == 5:\n",
    "        print(\"Variant overall: \", rms, tree_reg.feature_importances_)\n",
    "    else:\n",
    "        print(\"Variant {0}: \".format(i + 1), rms, tree_reg.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.52922905258934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([37, 14,  9, 39,  4, 19, 31, 43, 52, 50, 38, 47, 42, 44, 46, 51, 48,\n",
       "       25, 45, 41, 15, 40, 21, 49, 16,  8, 23, 26,  7, 24, 22, 10, 12, 13,\n",
       "       27, 17,  3, 29, 18,  0, 20,  1, 28,  2, 32, 33,  5, 35, 11,  6, 36,\n",
       "       34, 30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(tree_reg.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: max_depth\n",
    "4 63.24391058464195\n",
    "5 68.8412014560354\n",
    "7 81.84206024303708\n",
    "10 76.01870207652473\n",
    "12 60.52922905258934\n",
    "15 70.74621343597727"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 1:  63.35019782228598 [1.04120609e-02 8.88918776e-03 8.75750945e-03 1.03695757e-02\n",
      " 1.94965915e-03 7.90631750e-03 4.45927551e-03 1.00763012e-02\n",
      " 7.34656703e-03 5.75121888e-04 6.48337785e-03 1.09773492e-02\n",
      " 1.32092242e-02 1.54969976e-02 4.31420290e-07 1.26533657e-02\n",
      " 9.27700542e-03 8.22446180e-03 8.14333170e-03 1.75500565e-03\n",
      " 4.47292742e-03 6.28970886e-03 7.41260794e-03 1.06846444e-02\n",
      " 1.12420334e-02 5.76763571e-03 1.56459326e-02 6.65242039e-03\n",
      " 4.35883968e-03 2.26810686e-02 2.50950148e-01 5.63337418e-04\n",
      " 2.60663916e-02 6.84965116e-02 2.27642547e-01 1.76485229e-02\n",
      " 1.28522853e-01 0.00000000e+00 3.00870368e-04 1.97517517e-03\n",
      " 1.20997943e-03 1.68555788e-03 3.51505760e-03 1.51341246e-03\n",
      " 2.50912596e-03 1.20775783e-03 1.72410852e-03 1.68306227e-03\n",
      " 1.21418369e-03 5.03816607e-03 1.78176739e-03 1.30679755e-03\n",
      " 1.27472000e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 2:  64.77159501851575 [1.32767562e-02 7.31102465e-03 3.07401277e-02 8.16170720e-03\n",
      " 5.81650179e-04 7.52721584e-03 1.97764246e-02 1.48015014e-02\n",
      " 9.09419629e-03 3.38644802e-04 6.10817140e-03 1.35996969e-02\n",
      " 1.53312774e-02 8.75658652e-03 1.04157473e-07 5.81446683e-03\n",
      " 1.78038260e-02 1.20090193e-02 8.58422581e-03 3.10136873e-04\n",
      " 3.42483515e-03 2.68760050e-02 1.73995555e-02 6.91811291e-03\n",
      " 1.64484267e-02 7.38599362e-03 1.36151932e-02 1.31017431e-02\n",
      " 6.79778547e-03 1.59195847e-02 2.39897921e-01 1.28692009e-03\n",
      " 1.78737002e-02 4.93733608e-02 2.12896180e-01 2.11049917e-02\n",
      " 1.01792734e-01 0.00000000e+00 4.40632671e-04 1.18775096e-03\n",
      " 2.37223609e-03 2.99976220e-03 2.01576467e-03 2.65113746e-03\n",
      " 1.09121312e-03 1.35559784e-03 1.19793591e-03 3.01576852e-03\n",
      " 2.64877785e-03 2.28967530e-03 2.17006234e-03 1.05914003e-03\n",
      " 1.46474224e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 3:  56.450809932794854 [1.35924410e-02 7.76944867e-03 1.14981988e-02 7.07646982e-03\n",
      " 4.48613513e-04 9.22506167e-03 2.15233192e-02 1.24457230e-02\n",
      " 7.15628180e-03 6.94273723e-04 3.77258071e-03 1.34367346e-02\n",
      " 9.96650046e-03 1.11537876e-02 1.94986847e-10 5.23983872e-03\n",
      " 3.19145003e-02 9.32421466e-03 6.18947844e-03 5.88011682e-04\n",
      " 5.52830718e-03 2.83755583e-02 1.37352758e-02 5.84676851e-03\n",
      " 5.41339939e-03 4.22466445e-03 1.29394495e-02 8.01339243e-03\n",
      " 1.10686704e-02 2.24898993e-02 2.72688256e-01 6.19701331e-04\n",
      " 2.32924475e-02 4.50886117e-02 2.05033414e-01 2.17171016e-02\n",
      " 9.83408005e-02 0.00000000e+00 3.59897977e-04 1.90439285e-03\n",
      " 2.75631733e-03 2.66269143e-03 1.25990961e-03 1.55169067e-03\n",
      " 3.37822810e-03 1.14756065e-03 1.67062339e-03 1.99840645e-03\n",
      " 6.42239558e-03 1.49291761e-03 1.55363457e-03 3.22981510e-03\n",
      " 1.18032199e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 4:  75.37738951095992 [1.57246638e-02 7.58738236e-03 1.36607351e-02 9.90262163e-03\n",
      " 8.61726020e-04 1.45401146e-02 6.77216936e-03 1.55468392e-02\n",
      " 1.11748356e-02 8.16694091e-04 4.45127931e-03 1.49320663e-02\n",
      " 1.07438858e-02 1.40908581e-02 4.91931514e-10 6.41978694e-03\n",
      " 1.36632035e-02 8.46461530e-03 2.20569334e-02 7.22555570e-04\n",
      " 6.65534636e-03 1.28481875e-02 1.29553652e-02 7.19052942e-03\n",
      " 9.33672530e-03 4.97329899e-03 1.57551088e-02 9.14799305e-03\n",
      " 1.11108567e-02 1.64549843e-02 2.84547876e-01 5.39082015e-04\n",
      " 2.04740608e-02 2.29441560e-02 2.01279943e-01 3.02615171e-02\n",
      " 1.11102453e-01 0.00000000e+00 4.29224013e-04 1.22313791e-03\n",
      " 2.10666295e-03 3.67241392e-03 2.40301377e-03 1.67840989e-03\n",
      " 2.00711979e-03 1.83566697e-03 1.45020276e-03 1.85391515e-03\n",
      " 2.48898019e-03 3.80238911e-03 1.77350585e-03 1.74444053e-03\n",
      " 1.82046701e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 5:  63.82691182361886 [1.31549804e-02 9.39310153e-03 1.03746489e-02 7.74811097e-03\n",
      " 3.01992697e-04 1.32340989e-02 2.28671868e-02 1.16326714e-02\n",
      " 8.62394830e-03 5.05704054e-04 3.38396656e-03 2.65665126e-02\n",
      " 1.20102296e-02 9.24428142e-03 1.98467211e-07 3.39653658e-03\n",
      " 2.28248521e-02 7.56950839e-03 9.74325217e-03 3.35265341e-04\n",
      " 5.25398958e-03 2.03977254e-02 1.03222648e-02 4.32081903e-03\n",
      " 7.50873272e-03 4.31490083e-03 1.06413339e-02 5.70324227e-03\n",
      " 8.79408103e-03 2.50004021e-02 2.86376738e-01 6.39404564e-04\n",
      " 2.38951028e-02 3.78325068e-02 2.15406734e-01 2.45615152e-02\n",
      " 8.46863422e-02 0.00000000e+00 3.44772531e-04 1.79361530e-03\n",
      " 2.65188002e-03 3.58843322e-03 2.37576722e-03 1.28896950e-03\n",
      " 1.50946491e-03 1.86978773e-03 1.90326323e-03 1.93482920e-03\n",
      " 1.89754395e-03 3.58005154e-03 2.63179340e-03 1.72025890e-03\n",
      " 2.34268671e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant overall:  63.82691182361886 [1.31549804e-02 9.39310153e-03 1.03746489e-02 7.74811097e-03\n",
      " 3.01992697e-04 1.32340989e-02 2.28671868e-02 1.16326714e-02\n",
      " 8.62394830e-03 5.05704054e-04 3.38396656e-03 2.65665126e-02\n",
      " 1.20102296e-02 9.24428142e-03 1.98467211e-07 3.39653658e-03\n",
      " 2.28248521e-02 7.56950839e-03 9.74325217e-03 3.35265341e-04\n",
      " 5.25398958e-03 2.03977254e-02 1.03222648e-02 4.32081903e-03\n",
      " 7.50873272e-03 4.31490083e-03 1.06413339e-02 5.70324227e-03\n",
      " 8.79408103e-03 2.50004021e-02 2.86376738e-01 6.39404564e-04\n",
      " 2.38951028e-02 3.78325068e-02 2.15406734e-01 2.45615152e-02\n",
      " 8.46863422e-02 0.00000000e+00 3.44772531e-04 1.79361530e-03\n",
      " 2.65188002e-03 3.58843322e-03 2.37576722e-03 1.28896950e-03\n",
      " 1.50946491e-03 1.86978773e-03 1.90326323e-03 1.93482920e-03\n",
      " 1.89754395e-03 3.58005154e-03 2.63179340e-03 1.72025890e-03\n",
      " 2.34268671e-03]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rnd_clf = RandomForestRegressor(max_depth=8, random_state=0,\n",
    "                                  n_estimators=100)\n",
    "\n",
    "for i in range(6):\n",
    "    rnd_clf.fit(X_train_list[i], y_train_list[i])\n",
    "    y_pred_rf = rnd_clf.predict(X_test)\n",
    "    rms = sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "    \n",
    "    if i == 5:\n",
    "        print(\"Variant overall: \", rms, rnd_clf.feature_importances_)\n",
    "    else:\n",
    "        print(\"Variant {0}: \".format(i + 1), rms, rnd_clf.feature_importances_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rnd_clf = RandomForestRegressor(max_depth=8, random_state=0,\n",
    "                                  n_estimators=100)\n",
    "rnd_clf.fit(X_train_all, y_train_all)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: \n",
    "5 61.20775368071681\n",
    "7 58.97565854598216\n",
    "8 57.8040005639679\n",
    "9 59.16602957537276\n",
    "10 58.721486172923015\n",
    "15 59.89968118558552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 37, 31, 39, 46, 51, 19, 38, 44,  9, 47, 52, 43,  4, 45, 50, 40,\n",
       "       42, 49, 48, 15, 41, 25, 10, 27, 24, 26, 20, 17, 23,  0,  3, 13, 12,\n",
       "        1, 18,  8, 11, 35,  2,  7, 22,  5, 28, 16, 32, 21, 29, 33,  6, 36,\n",
       "       34, 30])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(rnd_clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 1:  72.08202510646414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 2:  132.09428657080952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 3:  62.42685200771992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 4:  69.97602517694499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant 5:  65.0001545415923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variant overall:  82.90077871909423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=6, n_estimators=3, learning_rate=1.0, random_state=42)\n",
    "\n",
    "for i in range(6):\n",
    "    gbrt.fit(X_train_list[i], y_train_list[i])\n",
    "    y_predicted = gbrt.predict(X_test)\n",
    "    rms = sqrt(mean_squared_error(y_test, y_predicted))\n",
    "    \n",
    "    if i == 5:\n",
    "        print(\"Variant overall: \", rms)\n",
    "    else:\n",
    "        print(\"Variant {0}: \".format(i + 1), rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training a Multi-Layer Perceptron (MLP)\n",
    "- inputs:\n",
    "    - x: set of features/attributes(array size: (n_samples,n_features))\n",
    "    - y: target values/class labels(array size: (n_samples,))\n",
    "    - k: number of folds of cross validation (10 as default)\n",
    "- outputs:\n",
    "    - regressor: the MLP regressor (object)\n",
    "    - scaler: the scaling of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:1316: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 525.52763257\n",
      "Validation score: 0.295081\n",
      "Iteration 2, loss = 431.02174094\n",
      "Validation score: 0.341772\n",
      "Iteration 3, loss = 415.82160481\n",
      "Validation score: 0.356275\n",
      "Iteration 4, loss = 406.37237582\n",
      "Validation score: 0.369474\n",
      "Iteration 5, loss = 396.13467152\n",
      "Validation score: 0.385910\n",
      "Iteration 6, loss = 386.17967873\n",
      "Validation score: 0.373560\n",
      "Iteration 7, loss = 382.04747864\n",
      "Validation score: 0.408482\n",
      "Iteration 8, loss = 370.12045260\n",
      "Validation score: 0.417381\n",
      "Iteration 9, loss = 361.69566521\n",
      "Validation score: 0.433801\n",
      "Iteration 10, loss = 354.23221095\n",
      "Validation score: 0.454335\n",
      "Iteration 11, loss = 351.30934859\n",
      "Validation score: 0.460465\n",
      "Iteration 12, loss = 339.85922220\n",
      "Validation score: 0.477770\n",
      "Iteration 13, loss = 333.48864344\n",
      "Validation score: 0.486766\n",
      "Iteration 14, loss = 325.64216777\n",
      "Validation score: 0.482897\n",
      "Iteration 15, loss = 321.47563372\n",
      "Validation score: 0.491466\n",
      "Iteration 16, loss = 314.21717725\n",
      "Validation score: 0.506830\n",
      "Iteration 17, loss = 314.98709932\n",
      "Validation score: 0.513919\n",
      "Iteration 18, loss = 304.60067185\n",
      "Validation score: 0.516834\n",
      "Iteration 19, loss = 297.38044477\n",
      "Validation score: 0.516095\n",
      "Iteration 20, loss = 294.07546602\n",
      "Validation score: 0.530173\n",
      "Iteration 21, loss = 289.48377047\n",
      "Validation score: 0.545572\n",
      "Iteration 22, loss = 283.38922057\n",
      "Validation score: 0.533583\n",
      "Iteration 23, loss = 281.06020430\n",
      "Validation score: 0.522549\n",
      "Iteration 24, loss = 277.89652111\n",
      "Validation score: 0.546570\n",
      "Iteration 25, loss = 274.44599317\n",
      "Validation score: 0.541035\n",
      "Iteration 26, loss = 274.01778979\n",
      "Validation score: 0.547005\n",
      "Iteration 27, loss = 266.69943003\n",
      "Validation score: 0.549348\n",
      "Iteration 28, loss = 269.00515425\n",
      "Validation score: 0.557500\n",
      "Iteration 29, loss = 259.78417759\n",
      "Validation score: 0.522656\n",
      "Iteration 30, loss = 260.68984419\n",
      "Validation score: 0.556485\n",
      "Iteration 31, loss = 253.03978535\n",
      "Validation score: 0.552327\n",
      "Iteration 32, loss = 251.82034009\n",
      "Validation score: 0.564702\n",
      "Iteration 33, loss = 253.22095260\n",
      "Validation score: 0.566840\n",
      "Iteration 34, loss = 249.17795282\n",
      "Validation score: 0.562118\n",
      "Iteration 35, loss = 245.81881524\n",
      "Validation score: 0.557627\n",
      "Iteration 36, loss = 242.12390190\n",
      "Validation score: 0.564832\n",
      "Iteration 37, loss = 243.14950694\n",
      "Validation score: 0.552791\n",
      "Iteration 38, loss = 241.37421633\n",
      "Validation score: 0.573382\n",
      "Iteration 39, loss = 231.82440566\n",
      "Validation score: 0.570213\n",
      "Iteration 40, loss = 233.46096872\n",
      "Validation score: 0.573001\n",
      "Iteration 41, loss = 230.26076530\n",
      "Validation score: 0.560852\n",
      "Iteration 42, loss = 233.45174181\n",
      "Validation score: 0.537986\n",
      "Iteration 43, loss = 228.42293747\n",
      "Validation score: 0.575284\n",
      "Iteration 44, loss = 224.26615126\n",
      "Validation score: 0.571690\n",
      "Iteration 45, loss = 223.60583197\n",
      "Validation score: 0.566046\n",
      "Iteration 46, loss = 221.94250389\n",
      "Validation score: 0.563228\n",
      "Iteration 47, loss = 221.16529720\n",
      "Validation score: 0.555370\n",
      "Iteration 48, loss = 220.27273104\n",
      "Validation score: 0.578929\n",
      "Iteration 49, loss = 213.14478901\n",
      "Validation score: 0.559926\n",
      "Iteration 50, loss = 213.24214333\n",
      "Validation score: 0.596327\n",
      "Iteration 51, loss = 210.99534661\n",
      "Validation score: 0.570187\n",
      "Iteration 52, loss = 210.57858323\n",
      "Validation score: 0.566933\n",
      "Iteration 53, loss = 213.66312176\n",
      "Validation score: 0.579847\n",
      "Iteration 54, loss = 209.27502151\n",
      "Validation score: 0.578906\n",
      "Iteration 55, loss = 205.80793844\n",
      "Validation score: 0.574137\n",
      "Iteration 56, loss = 202.35115499\n",
      "Validation score: 0.551905\n",
      "Iteration 57, loss = 207.85858623\n",
      "Validation score: 0.570846\n",
      "Iteration 58, loss = 208.17893951\n",
      "Validation score: 0.568460\n",
      "Iteration 59, loss = 206.10167972\n",
      "Validation score: 0.583089\n",
      "Iteration 60, loss = 198.97004886\n",
      "Validation score: 0.565277\n",
      "Iteration 61, loss = 195.65521688\n",
      "Validation score: 0.576433\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Variant 1:  82.59777194879051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:1316: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 508.86816898\n",
      "Validation score: 0.335793\n",
      "Iteration 2, loss = 425.57450072\n",
      "Validation score: 0.344854\n",
      "Iteration 3, loss = 409.08138440\n",
      "Validation score: 0.381878\n",
      "Iteration 4, loss = 392.17346668\n",
      "Validation score: 0.414270\n",
      "Iteration 5, loss = 381.90249689\n",
      "Validation score: 0.440164\n",
      "Iteration 6, loss = 365.50410301\n",
      "Validation score: 0.419253\n",
      "Iteration 7, loss = 356.49760537\n",
      "Validation score: 0.444655\n",
      "Iteration 8, loss = 345.40246715\n",
      "Validation score: 0.476499\n",
      "Iteration 9, loss = 338.45670499\n",
      "Validation score: 0.480274\n",
      "Iteration 10, loss = 327.48235733\n",
      "Validation score: 0.443865\n",
      "Iteration 11, loss = 322.41127067\n",
      "Validation score: 0.489523\n",
      "Iteration 12, loss = 312.25815680\n",
      "Validation score: 0.496293\n",
      "Iteration 13, loss = 309.19765331\n",
      "Validation score: 0.513057\n",
      "Iteration 14, loss = 297.06992716\n",
      "Validation score: 0.457981\n",
      "Iteration 15, loss = 300.32944919\n",
      "Validation score: 0.484840\n",
      "Iteration 16, loss = 289.59995866\n",
      "Validation score: 0.521055\n",
      "Iteration 17, loss = 283.70855689\n",
      "Validation score: 0.492853\n",
      "Iteration 18, loss = 280.79061063\n",
      "Validation score: 0.530774\n",
      "Iteration 19, loss = 273.67468401\n",
      "Validation score: 0.507475\n",
      "Iteration 20, loss = 267.46243188\n",
      "Validation score: 0.511160\n",
      "Iteration 21, loss = 269.62892863\n",
      "Validation score: 0.510921\n",
      "Iteration 22, loss = 263.88267832\n",
      "Validation score: 0.522813\n",
      "Iteration 23, loss = 257.24084869\n",
      "Validation score: 0.561070\n",
      "Iteration 24, loss = 252.56621983\n",
      "Validation score: 0.565059\n",
      "Iteration 25, loss = 250.70693137\n",
      "Validation score: 0.521428\n",
      "Iteration 26, loss = 249.90292228\n",
      "Validation score: 0.548139\n",
      "Iteration 27, loss = 243.45712364\n",
      "Validation score: 0.582339\n",
      "Iteration 28, loss = 243.02079509\n",
      "Validation score: 0.576526\n",
      "Iteration 29, loss = 238.34702205\n",
      "Validation score: 0.566416\n",
      "Iteration 30, loss = 235.65420784\n",
      "Validation score: 0.575870\n",
      "Iteration 31, loss = 235.90469812\n",
      "Validation score: 0.558138\n",
      "Iteration 32, loss = 233.65282245\n",
      "Validation score: 0.533443\n",
      "Iteration 33, loss = 232.21702520\n",
      "Validation score: 0.444043\n",
      "Iteration 34, loss = 232.39350075\n",
      "Validation score: 0.562449\n",
      "Iteration 35, loss = 222.31300685\n",
      "Validation score: 0.582115\n",
      "Iteration 36, loss = 229.57412441\n",
      "Validation score: 0.590649\n",
      "Iteration 37, loss = 224.00655846\n",
      "Validation score: 0.571141\n",
      "Iteration 38, loss = 221.03487717\n",
      "Validation score: 0.598712\n",
      "Iteration 39, loss = 215.81282291\n",
      "Validation score: 0.604438\n",
      "Iteration 40, loss = 218.05177746\n",
      "Validation score: 0.597420\n",
      "Iteration 41, loss = 212.14131982\n",
      "Validation score: 0.548511\n",
      "Iteration 42, loss = 210.03161406\n",
      "Validation score: 0.596513\n",
      "Iteration 43, loss = 209.35900832\n",
      "Validation score: 0.533967\n",
      "Iteration 44, loss = 207.75130122\n",
      "Validation score: 0.580223\n",
      "Iteration 45, loss = 202.97432852\n",
      "Validation score: 0.595780\n",
      "Iteration 46, loss = 205.91703514\n",
      "Validation score: 0.605028\n",
      "Iteration 47, loss = 202.95880104\n",
      "Validation score: 0.558907\n",
      "Iteration 48, loss = 202.20079772\n",
      "Validation score: 0.559985\n",
      "Iteration 49, loss = 207.72233734\n",
      "Validation score: 0.617126\n",
      "Iteration 50, loss = 197.02046175\n",
      "Validation score: 0.589878\n",
      "Iteration 51, loss = 197.34983409\n",
      "Validation score: 0.611119\n",
      "Iteration 52, loss = 195.53251489\n",
      "Validation score: 0.572768\n",
      "Iteration 53, loss = 200.38398910\n",
      "Validation score: 0.549862\n",
      "Iteration 54, loss = 189.15584115\n",
      "Validation score: 0.583184\n",
      "Iteration 55, loss = 195.46407449\n",
      "Validation score: 0.608940\n",
      "Iteration 56, loss = 193.28769739\n",
      "Validation score: 0.602872\n",
      "Iteration 57, loss = 190.38913483\n",
      "Validation score: 0.499998\n",
      "Iteration 58, loss = 201.27811085\n",
      "Validation score: 0.610938\n",
      "Iteration 59, loss = 189.08201342\n",
      "Validation score: 0.607025\n",
      "Iteration 60, loss = 187.50590629\n",
      "Validation score: 0.596872\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Variant 2:  81.42184618135359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:1316: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 401.87076046\n",
      "Validation score: 0.461332\n",
      "Iteration 2, loss = 337.54384928\n",
      "Validation score: 0.480620\n",
      "Iteration 3, loss = 319.68707244\n",
      "Validation score: 0.528708\n",
      "Iteration 4, loss = 300.11345379\n",
      "Validation score: 0.516101\n",
      "Iteration 5, loss = 285.60333404\n",
      "Validation score: 0.562061\n",
      "Iteration 6, loss = 270.87014862\n",
      "Validation score: 0.587380\n",
      "Iteration 7, loss = 262.28182970\n",
      "Validation score: 0.585507\n",
      "Iteration 8, loss = 258.64083353\n",
      "Validation score: 0.607313\n",
      "Iteration 9, loss = 245.50768625\n",
      "Validation score: 0.626802\n",
      "Iteration 10, loss = 238.71595340\n",
      "Validation score: 0.611780\n",
      "Iteration 11, loss = 233.99009715\n",
      "Validation score: 0.621132\n",
      "Iteration 12, loss = 232.41842872\n",
      "Validation score: 0.638236\n",
      "Iteration 13, loss = 224.43046534\n",
      "Validation score: 0.639443\n",
      "Iteration 14, loss = 217.01718992\n",
      "Validation score: 0.638738\n",
      "Iteration 15, loss = 211.93410848\n",
      "Validation score: 0.651339\n",
      "Iteration 16, loss = 211.07751275\n",
      "Validation score: 0.655508\n",
      "Iteration 17, loss = 201.38003479\n",
      "Validation score: 0.656120\n",
      "Iteration 18, loss = 198.82977654\n",
      "Validation score: 0.669864\n",
      "Iteration 19, loss = 195.40282575\n",
      "Validation score: 0.654614\n",
      "Iteration 20, loss = 197.53532525\n",
      "Validation score: 0.527474\n",
      "Iteration 21, loss = 210.20662948\n",
      "Validation score: 0.652738\n",
      "Iteration 22, loss = 187.64932639\n",
      "Validation score: 0.647635\n",
      "Iteration 23, loss = 186.30604573\n",
      "Validation score: 0.680554\n",
      "Iteration 24, loss = 185.28067542\n",
      "Validation score: 0.686674\n",
      "Iteration 25, loss = 183.49235818\n",
      "Validation score: 0.680071\n",
      "Iteration 26, loss = 181.60871806\n",
      "Validation score: 0.687585\n",
      "Iteration 27, loss = 179.25081542\n",
      "Validation score: 0.655757\n",
      "Iteration 28, loss = 180.17225070\n",
      "Validation score: 0.698942\n",
      "Iteration 29, loss = 175.17207020\n",
      "Validation score: 0.663739\n",
      "Iteration 30, loss = 175.88028211\n",
      "Validation score: 0.694590\n",
      "Iteration 31, loss = 169.64436358\n",
      "Validation score: 0.665421\n",
      "Iteration 32, loss = 169.38988641\n",
      "Validation score: 0.692762\n",
      "Iteration 33, loss = 166.84975956\n",
      "Validation score: 0.696604\n",
      "Iteration 34, loss = 162.34810909\n",
      "Validation score: 0.702423\n",
      "Iteration 35, loss = 164.98969689\n",
      "Validation score: 0.687284\n",
      "Iteration 36, loss = 166.15688446\n",
      "Validation score: 0.665509\n",
      "Iteration 37, loss = 163.76371860\n",
      "Validation score: 0.646053\n",
      "Iteration 38, loss = 168.12095448\n",
      "Validation score: 0.642632\n",
      "Iteration 39, loss = 157.88763078\n",
      "Validation score: 0.687743\n",
      "Iteration 40, loss = 154.97946977\n",
      "Validation score: 0.701279\n",
      "Iteration 41, loss = 156.11191910\n",
      "Validation score: 0.707884\n",
      "Iteration 42, loss = 151.87104316\n",
      "Validation score: 0.679167\n",
      "Iteration 43, loss = 156.67214308\n",
      "Validation score: 0.697080\n",
      "Iteration 44, loss = 156.99266708\n",
      "Validation score: 0.685185\n",
      "Iteration 45, loss = 152.89906300\n",
      "Validation score: 0.727212\n",
      "Iteration 46, loss = 161.50412532\n",
      "Validation score: 0.663098\n",
      "Iteration 47, loss = 154.24366587\n",
      "Validation score: 0.705434\n",
      "Iteration 48, loss = 152.80542402\n",
      "Validation score: 0.721528\n",
      "Iteration 49, loss = 146.07394982\n",
      "Validation score: 0.684644\n",
      "Iteration 50, loss = 144.68860435\n",
      "Validation score: 0.686112\n",
      "Iteration 51, loss = 148.35037712\n",
      "Validation score: 0.703913\n",
      "Iteration 52, loss = 145.82752344\n",
      "Validation score: 0.712999\n",
      "Iteration 53, loss = 150.49768393\n",
      "Validation score: 0.679886\n",
      "Iteration 54, loss = 146.56746023\n",
      "Validation score: 0.710282\n",
      "Iteration 55, loss = 141.29230356\n",
      "Validation score: 0.695958\n",
      "Iteration 56, loss = 141.13159094\n",
      "Validation score: 0.729348\n",
      "Iteration 57, loss = 140.90933145\n",
      "Validation score: 0.725030\n",
      "Iteration 58, loss = 142.20446723\n",
      "Validation score: 0.722404\n",
      "Iteration 59, loss = 138.01362026\n",
      "Validation score: 0.714797\n",
      "Iteration 60, loss = 139.18433787\n",
      "Validation score: 0.728795\n",
      "Iteration 61, loss = 141.94165672\n",
      "Validation score: 0.682609\n",
      "Iteration 62, loss = 149.41741934\n",
      "Validation score: 0.728545\n",
      "Iteration 63, loss = 137.35611663\n",
      "Validation score: 0.725884\n",
      "Iteration 64, loss = 140.00378811\n",
      "Validation score: 0.730684\n",
      "Iteration 65, loss = 135.57066332\n",
      "Validation score: 0.725391\n",
      "Iteration 66, loss = 132.39220142\n",
      "Validation score: 0.672380\n",
      "Iteration 67, loss = 134.75901878\n",
      "Validation score: 0.727482\n",
      "Iteration 68, loss = 135.38653345\n",
      "Validation score: 0.730657\n",
      "Iteration 69, loss = 133.66281739\n",
      "Validation score: 0.710168\n",
      "Iteration 70, loss = 140.69579122\n",
      "Validation score: 0.736158\n",
      "Iteration 71, loss = 132.90829811\n",
      "Validation score: 0.726963\n",
      "Iteration 72, loss = 136.48578722\n",
      "Validation score: 0.739643\n",
      "Iteration 73, loss = 131.73729874\n",
      "Validation score: 0.730325\n",
      "Iteration 74, loss = 131.03841342\n",
      "Validation score: 0.732309\n",
      "Iteration 75, loss = 129.81936918\n",
      "Validation score: 0.720367\n",
      "Iteration 76, loss = 131.25046148\n",
      "Validation score: 0.724176\n",
      "Iteration 77, loss = 130.55373578\n",
      "Validation score: 0.684248\n",
      "Iteration 78, loss = 129.28541002\n",
      "Validation score: 0.727278\n",
      "Iteration 79, loss = 128.34757901\n",
      "Validation score: 0.731626\n",
      "Iteration 80, loss = 127.53551073\n",
      "Validation score: 0.719173\n",
      "Iteration 81, loss = 129.11364941\n",
      "Validation score: 0.720895\n",
      "Iteration 82, loss = 128.39468638\n",
      "Validation score: 0.686286\n",
      "Iteration 83, loss = 125.25625412\n",
      "Validation score: 0.663678\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Variant 3:  83.6489248379533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:1316: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 405.62439505\n",
      "Validation score: 0.371167\n",
      "Iteration 2, loss = 351.64424888\n",
      "Validation score: 0.411997\n",
      "Iteration 3, loss = 330.95562848\n",
      "Validation score: 0.440500\n",
      "Iteration 4, loss = 312.06091440\n",
      "Validation score: 0.448668\n",
      "Iteration 5, loss = 299.02686686\n",
      "Validation score: 0.489178\n",
      "Iteration 6, loss = 286.10441461\n",
      "Validation score: 0.506623\n",
      "Iteration 7, loss = 278.02034676\n",
      "Validation score: 0.516387\n",
      "Iteration 8, loss = 270.83368854\n",
      "Validation score: 0.509479\n",
      "Iteration 9, loss = 266.01322303\n",
      "Validation score: 0.501708\n",
      "Iteration 10, loss = 257.87913776\n",
      "Validation score: 0.521001\n",
      "Iteration 11, loss = 252.54598250\n",
      "Validation score: 0.537035\n",
      "Iteration 12, loss = 248.53532401\n",
      "Validation score: 0.496001\n",
      "Iteration 13, loss = 240.32382012\n",
      "Validation score: 0.551253\n",
      "Iteration 14, loss = 234.40872814\n",
      "Validation score: 0.549583\n",
      "Iteration 15, loss = 234.27936895\n",
      "Validation score: 0.542511\n",
      "Iteration 16, loss = 228.60914873\n",
      "Validation score: 0.532776\n",
      "Iteration 17, loss = 220.52692776\n",
      "Validation score: 0.503678\n",
      "Iteration 18, loss = 220.96735538\n",
      "Validation score: 0.560993\n",
      "Iteration 19, loss = 215.09996997\n",
      "Validation score: 0.550979\n",
      "Iteration 20, loss = 213.41840175\n",
      "Validation score: 0.561281\n",
      "Iteration 21, loss = 208.32815786\n",
      "Validation score: 0.539140\n",
      "Iteration 22, loss = 201.85834824\n",
      "Validation score: 0.563634\n",
      "Iteration 23, loss = 205.03492275\n",
      "Validation score: 0.500673\n",
      "Iteration 24, loss = 202.81734304\n",
      "Validation score: 0.509771\n",
      "Iteration 25, loss = 194.11414530\n",
      "Validation score: 0.569943\n",
      "Iteration 26, loss = 193.42594344\n",
      "Validation score: 0.581512\n",
      "Iteration 27, loss = 192.12096088\n",
      "Validation score: 0.575654\n",
      "Iteration 28, loss = 187.21276928\n",
      "Validation score: 0.554275\n",
      "Iteration 29, loss = 184.74362269\n",
      "Validation score: 0.579340\n",
      "Iteration 30, loss = 184.00375589\n",
      "Validation score: 0.577819\n",
      "Iteration 31, loss = 179.23177451\n",
      "Validation score: 0.571312\n",
      "Iteration 32, loss = 177.72965528\n",
      "Validation score: 0.578375\n",
      "Iteration 33, loss = 176.58205185\n",
      "Validation score: 0.574051\n",
      "Iteration 34, loss = 173.14606891\n",
      "Validation score: 0.589968\n",
      "Iteration 35, loss = 175.79897253\n",
      "Validation score: 0.589449\n",
      "Iteration 36, loss = 172.35772320\n",
      "Validation score: 0.576845\n",
      "Iteration 37, loss = 167.52799353\n",
      "Validation score: 0.558758\n",
      "Iteration 38, loss = 167.61011412\n",
      "Validation score: 0.499346\n",
      "Iteration 39, loss = 164.11081218\n",
      "Validation score: 0.584597\n",
      "Iteration 40, loss = 162.41320761\n",
      "Validation score: 0.593829\n",
      "Iteration 41, loss = 163.22646239\n",
      "Validation score: 0.509630\n",
      "Iteration 42, loss = 161.72817987\n",
      "Validation score: 0.590546\n",
      "Iteration 43, loss = 159.64082605\n",
      "Validation score: 0.543821\n",
      "Iteration 44, loss = 157.71469546\n",
      "Validation score: 0.586525\n",
      "Iteration 45, loss = 159.38196938\n",
      "Validation score: 0.580425\n",
      "Iteration 46, loss = 158.79017449\n",
      "Validation score: 0.588789\n",
      "Iteration 47, loss = 155.10841453\n",
      "Validation score: 0.577511\n",
      "Iteration 48, loss = 153.58535948\n",
      "Validation score: 0.584744\n",
      "Iteration 49, loss = 155.57806223\n",
      "Validation score: 0.600484\n",
      "Iteration 50, loss = 146.58116365\n",
      "Validation score: 0.591149\n",
      "Iteration 51, loss = 152.94922157\n",
      "Validation score: 0.557665\n",
      "Iteration 52, loss = 149.98118220\n",
      "Validation score: 0.588556\n",
      "Iteration 53, loss = 152.96513179\n",
      "Validation score: 0.595463\n",
      "Iteration 54, loss = 148.38526146\n",
      "Validation score: 0.594791\n",
      "Iteration 55, loss = 146.39941830\n",
      "Validation score: 0.594562\n",
      "Iteration 56, loss = 147.70367647\n",
      "Validation score: 0.580151\n",
      "Iteration 57, loss = 151.31147223\n",
      "Validation score: 0.590834\n",
      "Iteration 58, loss = 147.09890972\n",
      "Validation score: 0.567669\n",
      "Iteration 59, loss = 145.10488313\n",
      "Validation score: 0.586454\n",
      "Iteration 60, loss = 142.53846662\n",
      "Validation score: 0.590818\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Variant 4:  361.2711681280831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:1316: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 395.77005317\n",
      "Validation score: 0.384385\n",
      "Iteration 2, loss = 340.11523842\n",
      "Validation score: 0.434836\n",
      "Iteration 3, loss = 313.84097268\n",
      "Validation score: 0.475388\n",
      "Iteration 4, loss = 300.22732083\n",
      "Validation score: 0.499796\n",
      "Iteration 5, loss = 286.27556535\n",
      "Validation score: 0.530974\n",
      "Iteration 6, loss = 277.47790190\n",
      "Validation score: 0.506087\n",
      "Iteration 7, loss = 268.35084149\n",
      "Validation score: 0.534633\n",
      "Iteration 8, loss = 264.87477582\n",
      "Validation score: 0.564952\n",
      "Iteration 9, loss = 258.21443957\n",
      "Validation score: 0.576547\n",
      "Iteration 10, loss = 250.95541247\n",
      "Validation score: 0.541921\n",
      "Iteration 11, loss = 248.85096743\n",
      "Validation score: 0.581770\n",
      "Iteration 12, loss = 243.08050729\n",
      "Validation score: 0.597276\n",
      "Iteration 13, loss = 242.64812854\n",
      "Validation score: 0.572713\n",
      "Iteration 14, loss = 238.59943355\n",
      "Validation score: 0.592818\n",
      "Iteration 15, loss = 232.75029510\n",
      "Validation score: 0.600768\n",
      "Iteration 16, loss = 230.04380052\n",
      "Validation score: 0.594725\n",
      "Iteration 17, loss = 229.76992840\n",
      "Validation score: 0.605884\n",
      "Iteration 18, loss = 226.78350210\n",
      "Validation score: 0.612516\n",
      "Iteration 19, loss = 220.92293205\n",
      "Validation score: 0.612470\n",
      "Iteration 20, loss = 220.62905162\n",
      "Validation score: 0.624925\n",
      "Iteration 21, loss = 217.18854741\n",
      "Validation score: 0.621261\n",
      "Iteration 22, loss = 217.35528874\n",
      "Validation score: 0.580866\n",
      "Iteration 23, loss = 213.72771919\n",
      "Validation score: 0.630909\n",
      "Iteration 24, loss = 213.92644458\n",
      "Validation score: 0.628303\n",
      "Iteration 25, loss = 211.48907730\n",
      "Validation score: 0.621457\n",
      "Iteration 26, loss = 205.24554742\n",
      "Validation score: 0.636914\n",
      "Iteration 27, loss = 205.59383664\n",
      "Validation score: 0.602030\n",
      "Iteration 28, loss = 205.79512063\n",
      "Validation score: 0.643988\n",
      "Iteration 29, loss = 203.58838794\n",
      "Validation score: 0.644944\n",
      "Iteration 30, loss = 199.64582842\n",
      "Validation score: 0.642867\n",
      "Iteration 31, loss = 203.34867066\n",
      "Validation score: 0.638991\n",
      "Iteration 32, loss = 201.01940667\n",
      "Validation score: 0.651458\n",
      "Iteration 33, loss = 199.04504166\n",
      "Validation score: 0.653240\n",
      "Iteration 34, loss = 193.96335462\n",
      "Validation score: 0.646482\n",
      "Iteration 35, loss = 193.31181483\n",
      "Validation score: 0.637174\n",
      "Iteration 36, loss = 193.68143281\n",
      "Validation score: 0.637943\n",
      "Iteration 37, loss = 193.06602677\n",
      "Validation score: 0.652850\n",
      "Iteration 38, loss = 190.92750220\n",
      "Validation score: 0.651380\n",
      "Iteration 39, loss = 192.52387628\n",
      "Validation score: 0.636280\n",
      "Iteration 40, loss = 188.60733469\n",
      "Validation score: 0.612018\n",
      "Iteration 41, loss = 190.36636538\n",
      "Validation score: 0.647220\n",
      "Iteration 42, loss = 184.09086461\n",
      "Validation score: 0.670116\n",
      "Iteration 43, loss = 186.59480363\n",
      "Validation score: 0.666672\n",
      "Iteration 44, loss = 180.68451547\n",
      "Validation score: 0.669387\n",
      "Iteration 45, loss = 182.70503973\n",
      "Validation score: 0.679148\n",
      "Iteration 46, loss = 181.57373217\n",
      "Validation score: 0.620805\n",
      "Iteration 47, loss = 181.12858205\n",
      "Validation score: 0.677235\n",
      "Iteration 48, loss = 181.41642539\n",
      "Validation score: 0.648300\n",
      "Iteration 49, loss = 179.05608913\n",
      "Validation score: 0.675188\n",
      "Iteration 50, loss = 177.14694061\n",
      "Validation score: 0.677933\n",
      "Iteration 51, loss = 175.86979080\n",
      "Validation score: 0.678921\n",
      "Iteration 52, loss = 179.95970010\n",
      "Validation score: 0.675396\n",
      "Iteration 53, loss = 174.92538324\n",
      "Validation score: 0.656061\n",
      "Iteration 54, loss = 172.30482481\n",
      "Validation score: 0.661831\n",
      "Iteration 55, loss = 171.49357436\n",
      "Validation score: 0.689593\n",
      "Iteration 56, loss = 174.82187097\n",
      "Validation score: 0.679307\n",
      "Iteration 57, loss = 169.49177093\n",
      "Validation score: 0.680212\n",
      "Iteration 58, loss = 169.75956043\n",
      "Validation score: 0.687490\n",
      "Iteration 59, loss = 167.92694386\n",
      "Validation score: 0.685472\n",
      "Iteration 60, loss = 169.83738139\n",
      "Validation score: 0.677020\n",
      "Iteration 61, loss = 165.61598371\n",
      "Validation score: 0.690295\n",
      "Iteration 62, loss = 169.10901991\n",
      "Validation score: 0.630573\n",
      "Iteration 63, loss = 168.23646737\n",
      "Validation score: 0.670265\n",
      "Iteration 64, loss = 163.00955727\n",
      "Validation score: 0.689016\n",
      "Iteration 65, loss = 166.19268897\n",
      "Validation score: 0.683992\n",
      "Iteration 66, loss = 164.13714513\n",
      "Validation score: 0.681066\n",
      "Iteration 67, loss = 161.05928444\n",
      "Validation score: 0.677652\n",
      "Iteration 68, loss = 161.66436792\n",
      "Validation score: 0.647936\n",
      "Iteration 69, loss = 164.17483688\n",
      "Validation score: 0.678008\n",
      "Iteration 70, loss = 157.79625409\n",
      "Validation score: 0.683998\n",
      "Iteration 71, loss = 157.80374720\n",
      "Validation score: 0.679587\n",
      "Iteration 72, loss = 157.82829087\n",
      "Validation score: 0.694001\n",
      "Iteration 73, loss = 160.50707276\n",
      "Validation score: 0.693852\n",
      "Iteration 74, loss = 157.39349089\n",
      "Validation score: 0.683617\n",
      "Iteration 75, loss = 158.94549963\n",
      "Validation score: 0.641525\n",
      "Iteration 76, loss = 157.81746023\n",
      "Validation score: 0.685062\n",
      "Iteration 77, loss = 154.54822653\n",
      "Validation score: 0.682561\n",
      "Iteration 78, loss = 154.14917235\n",
      "Validation score: 0.700492\n",
      "Iteration 79, loss = 154.61581436\n",
      "Validation score: 0.702638\n",
      "Iteration 80, loss = 149.43477896\n",
      "Validation score: 0.668954\n",
      "Iteration 81, loss = 154.77037966\n",
      "Validation score: 0.704084\n",
      "Iteration 82, loss = 153.77404755\n",
      "Validation score: 0.683608\n",
      "Iteration 83, loss = 148.81572415\n",
      "Validation score: 0.703469\n",
      "Iteration 84, loss = 149.98667704\n",
      "Validation score: 0.695399\n",
      "Iteration 85, loss = 151.35619123\n",
      "Validation score: 0.680251\n",
      "Iteration 86, loss = 149.08426362\n",
      "Validation score: 0.705862\n",
      "Iteration 87, loss = 147.59333143\n",
      "Validation score: 0.695510\n",
      "Iteration 88, loss = 146.80044841\n",
      "Validation score: 0.704991\n",
      "Iteration 89, loss = 146.03886820\n",
      "Validation score: 0.676444\n",
      "Iteration 90, loss = 148.45088022\n",
      "Validation score: 0.684900\n",
      "Iteration 91, loss = 147.15406634\n",
      "Validation score: 0.699373\n",
      "Iteration 92, loss = 144.24115012\n",
      "Validation score: 0.693790\n",
      "Iteration 93, loss = 144.41545779\n",
      "Validation score: 0.709327\n",
      "Iteration 94, loss = 144.28044032\n",
      "Validation score: 0.705206\n",
      "Iteration 95, loss = 144.13211525\n",
      "Validation score: 0.666831\n",
      "Iteration 96, loss = 141.41938132\n",
      "Validation score: 0.695229\n",
      "Iteration 97, loss = 139.84927805\n",
      "Validation score: 0.698986\n",
      "Iteration 98, loss = 141.60637527\n",
      "Validation score: 0.699109\n",
      "Iteration 99, loss = 146.84633660\n",
      "Validation score: 0.707455\n",
      "Iteration 100, loss = 141.70285711\n",
      "Validation score: 0.677904\n",
      "Iteration 101, loss = 145.72249928\n",
      "Validation score: 0.709089\n",
      "Iteration 102, loss = 138.38411459\n",
      "Validation score: 0.691696\n",
      "Iteration 103, loss = 140.11639546\n",
      "Validation score: 0.705720\n",
      "Iteration 104, loss = 138.65242116\n",
      "Validation score: 0.703187\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Variant 5:  83.01520983444237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:1316: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 359.47681068\n",
      "Validation score: 0.495548\n",
      "Iteration 2, loss = 294.38031560\n",
      "Validation score: 0.532208\n",
      "Iteration 3, loss = 267.31289546\n",
      "Validation score: 0.548850\n",
      "Iteration 4, loss = 251.36979589\n",
      "Validation score: 0.588507\n",
      "Iteration 5, loss = 241.28461228\n",
      "Validation score: 0.585489\n",
      "Iteration 6, loss = 232.03370584\n",
      "Validation score: 0.601702\n",
      "Iteration 7, loss = 223.56702092\n",
      "Validation score: 0.608840\n",
      "Iteration 8, loss = 215.60601234\n",
      "Validation score: 0.597483\n",
      "Iteration 9, loss = 208.59880956\n",
      "Validation score: 0.604175\n",
      "Iteration 10, loss = 204.33948050\n",
      "Validation score: 0.623606\n",
      "Iteration 11, loss = 200.08945549\n",
      "Validation score: 0.624466\n",
      "Iteration 12, loss = 196.27365170\n",
      "Validation score: 0.611747\n",
      "Iteration 13, loss = 194.39092816\n",
      "Validation score: 0.631335\n",
      "Iteration 14, loss = 189.15459226\n",
      "Validation score: 0.625598\n",
      "Iteration 15, loss = 190.11258114\n",
      "Validation score: 0.629453\n",
      "Iteration 16, loss = 185.40146474\n",
      "Validation score: 0.611660\n",
      "Iteration 17, loss = 185.85205823\n",
      "Validation score: 0.626718\n",
      "Iteration 18, loss = 183.61893943\n",
      "Validation score: 0.637099\n",
      "Iteration 19, loss = 181.81159372\n",
      "Validation score: 0.624505\n",
      "Iteration 20, loss = 181.44767274\n",
      "Validation score: 0.639322\n",
      "Iteration 21, loss = 178.26566199\n",
      "Validation score: 0.638052\n",
      "Iteration 22, loss = 176.30061828\n",
      "Validation score: 0.612510\n",
      "Iteration 23, loss = 174.92706401\n",
      "Validation score: 0.643183\n",
      "Iteration 24, loss = 172.85906841\n",
      "Validation score: 0.636195\n",
      "Iteration 25, loss = 171.92177163\n",
      "Validation score: 0.639829\n",
      "Iteration 26, loss = 171.94278560\n",
      "Validation score: 0.604909\n",
      "Iteration 27, loss = 169.28379515\n",
      "Validation score: 0.630262\n",
      "Iteration 28, loss = 169.55195551\n",
      "Validation score: 0.636354\n",
      "Iteration 29, loss = 167.42669277\n",
      "Validation score: 0.646906\n",
      "Iteration 30, loss = 168.26088450\n",
      "Validation score: 0.644479\n",
      "Iteration 31, loss = 162.87398006\n",
      "Validation score: 0.647871\n",
      "Iteration 32, loss = 165.86032145\n",
      "Validation score: 0.646550\n",
      "Iteration 33, loss = 164.06031076\n",
      "Validation score: 0.651672\n",
      "Iteration 34, loss = 162.35257592\n",
      "Validation score: 0.642896\n",
      "Iteration 35, loss = 163.09767462\n",
      "Validation score: 0.653035\n",
      "Iteration 36, loss = 161.97863686\n",
      "Validation score: 0.631808\n",
      "Iteration 37, loss = 159.43389537\n",
      "Validation score: 0.653338\n",
      "Iteration 38, loss = 158.63079831\n",
      "Validation score: 0.650544\n",
      "Iteration 39, loss = 158.78357274\n",
      "Validation score: 0.646405\n",
      "Iteration 40, loss = 156.26836945\n",
      "Validation score: 0.647232\n",
      "Iteration 41, loss = 158.33971707\n",
      "Validation score: 0.646579\n",
      "Iteration 42, loss = 158.29315192\n",
      "Validation score: 0.664611\n",
      "Iteration 43, loss = 155.21020594\n",
      "Validation score: 0.648239\n",
      "Iteration 44, loss = 154.06520361\n",
      "Validation score: 0.654189\n",
      "Iteration 45, loss = 154.52160266\n",
      "Validation score: 0.660587\n",
      "Iteration 46, loss = 152.18087825\n",
      "Validation score: 0.646299\n",
      "Iteration 47, loss = 152.36946528\n",
      "Validation score: 0.665270\n",
      "Iteration 48, loss = 152.03938921\n",
      "Validation score: 0.662787\n",
      "Iteration 49, loss = 149.79733706\n",
      "Validation score: 0.598330\n",
      "Iteration 50, loss = 149.56790009\n",
      "Validation score: 0.644691\n",
      "Iteration 51, loss = 147.20413595\n",
      "Validation score: 0.644007\n",
      "Iteration 52, loss = 149.10469991\n",
      "Validation score: 0.668517\n",
      "Iteration 53, loss = 149.57160727\n",
      "Validation score: 0.653067\n",
      "Iteration 54, loss = 146.39047409\n",
      "Validation score: 0.650674\n",
      "Iteration 55, loss = 146.53832704\n",
      "Validation score: 0.632102\n",
      "Iteration 56, loss = 146.32965546\n",
      "Validation score: 0.662191\n",
      "Iteration 57, loss = 144.58425881\n",
      "Validation score: 0.670028\n",
      "Iteration 58, loss = 146.08870923\n",
      "Validation score: 0.668488\n",
      "Iteration 59, loss = 143.39777726\n",
      "Validation score: 0.667689\n",
      "Iteration 60, loss = 143.17586876\n",
      "Validation score: 0.671302\n",
      "Iteration 61, loss = 143.31877457\n",
      "Validation score: 0.681025\n",
      "Iteration 62, loss = 143.05194401\n",
      "Validation score: 0.660681\n",
      "Iteration 63, loss = 140.69623262\n",
      "Validation score: 0.672076\n",
      "Iteration 64, loss = 144.31370078\n",
      "Validation score: 0.665578\n",
      "Iteration 65, loss = 139.50338160\n",
      "Validation score: 0.671946\n",
      "Iteration 66, loss = 140.54358230\n",
      "Validation score: 0.664089\n",
      "Iteration 67, loss = 137.39902942\n",
      "Validation score: 0.677856\n",
      "Iteration 68, loss = 140.07905764\n",
      "Validation score: 0.669705\n",
      "Iteration 69, loss = 138.05065352\n",
      "Validation score: 0.629869\n",
      "Iteration 70, loss = 136.32061798\n",
      "Validation score: 0.679425\n",
      "Iteration 71, loss = 137.02609917\n",
      "Validation score: 0.671349\n",
      "Iteration 72, loss = 136.41303980\n",
      "Validation score: 0.654567\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Variant overall:  111.21034538248148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_list[i])\n",
    "    MLPReg = MLPRegressor(alpha = 1e-4, hidden_layer_sizes = (150,5,), \n",
    "    random_state = 12, max_iter = 500, activation = 'relu',\n",
    "    verbose = True, early_stopping = True, learning_rate_init = 0.001)\n",
    "    mlp = MLPReg.fit(scaler.transform(X_train_list[i]), y_train_list[i])\n",
    "    predict_labels = mlp.predict(scaler.transform(X_test))\n",
    "    rms = sqrt(mean_squared_error(y_test, predict_labels))\n",
    "    if i == 5:\n",
    "        print(\"Variant overall: \", rms)\n",
    "    else:\n",
    "        print(\"Variant {0}: \".format(i + 1), rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
